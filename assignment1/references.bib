@article{reshef2011,
author = {David N. Reshef  and Yakir A. Reshef  and Hilary K. Finucane  and Sharon R. Grossman  and Gilean McVean  and Peter J. Turnbaugh  and Eric S. Lander  and Michael Mitzenmacher  and Pardis C. Sabeti },
title = {Detecting Novel Associations in Large Data Sets},
journal = {Science},
volume = {334},
number = {6062},
pages = {1518-1524},
year = {2011},
doi = {10.1126/science.1205438},
URL = {https://www.science.org/doi/abs/10.1126/science.1205438},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1205438},
abstract = {A statistical method reveals relationships among variables in complex data sets. Identifying interesting relationships between pairs of variables in large data sets is increasingly important. Here, we present a measure of dependence for two-variable relationships: the maximal information coefficient (MIC). MIC captures a wide range of associations both functional and not, and for functional relationships provides a score that roughly equals the coefficient of determination (R2) of the data relative to the regression function. MIC belongs to a larger class of maximal information-based nonparametric exploration (MINE) statistics for identifying and classifying relationships. We apply MIC and MINE to data sets in global health, gene expression, major-league baseball, and the human gut microbiota and identify known and novel relationships.}}


@misc{simon2014,
      title={Comment on "Detecting Novel Associations In Large Data Sets" by Reshef Et Al, Science Dec 16, 2011}, 
      author={Noah Simon and Robert Tibshirani},
      year={2014},
      eprint={1401.7645},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1401.7645}, 
}

@article{kinney2014,
author = {Justin B. Kinney  and Gurinder S. Atwal },
title = {Equitability, mutual information, and the maximal information coefficient},
journal = {Proceedings of the National Academy of Sciences},
volume = {111},
number = {9},
pages = {3354-3359},
year = {2014},
doi = {10.1073/pnas.1309933111},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1309933111},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1309933111},
abstract = {Attention has recently focused on a basic yet unresolved problem in statistics: How can one quantify the strength of a statistical association between two variables without bias for relationships of a specific form? Here we propose a way of mathematically formalizing this “equitability” criterion, using core concepts from information theory. This criterion is naturally satisfied by a fundamental information-theoretic measure of dependence called “mutual information.” By contrast, a recently introduced dependence measure called the “maximal information coefficient” is seen to violate equitability. We conclude that estimating mutual information provides a natural and practical method for equitably quantifying associations in large datasets. How should one quantify the strength of association between two random variables without bias for relationships of a specific form? Despite its conceptual simplicity, this notion of statistical “equitability” has yet to receive a definitive mathematical formalization. Here we argue that equitability is properly formalized by a self-consistency condition closely related to Data Processing Inequality. Mutual information, a fundamental quantity in information theory, is shown to satisfy this equitability criterion. These findings are at odds with the recent work of Reshef et al. [Reshef DN, et al. (2011) Science 334(6062):1518–1524], which proposed an alternative definition of equitability and introduced a new statistic, the “maximal information coefficient” (MIC), said to satisfy equitability in contradistinction to mutual information. These conclusions, however, were supported only with limited simulation evidence, not with mathematical arguments. Upon revisiting these claims, we prove that the mathematical definition of equitability proposed by Reshef et al. cannot be satisfied by any (nontrivial) dependence measure. We also identify artifacts in the reported simulation evidence. When these artifacts are removed, estimates of mutual information are found to be more equitable than estimates of MIC. Mutual information is also observed to have consistently higher statistical power than MIC. We conclude that estimating mutual information provides a natural (and often practical) way to equitably quantify statistical associations in large datasets.}}

@misc{src,
    title={Source code for the plots},
    author={Tymur Mykhalievskyi},
    year={2025},
    url={}}